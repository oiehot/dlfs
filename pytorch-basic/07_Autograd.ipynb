{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.autograd**는 PyTorch에서 기울기 값을 구하는 시스템이다.\n",
    "\n",
    "순전파 단계에서 autograd는:\n",
    "* 텐서의 forward 연산이 될 때 실행된다.\n",
    "* 연산 그래프를 동적으로 생성한다.\n",
    "* 텐서 연산(ex: add, matmul) 을 수행하여 결과를 텐서로 돌려준다.\n",
    "* 각 텐서의 `.grad_fn` 에 기울기 연산에 필요한 gradient function을 저장한다.\n",
    "\n",
    "역전파 단계에서 autograd는:\n",
    "* 결과 텐서 (Root)에서 .backward()가 호출될 때 시작한다.\n",
    "* 순전파 때 만들어진 연산 그래프의 역방향을 따라가며 연쇄적으로 실행된다.\n",
    "* `requires_grad=True`인 텐서에 대해서만 기울기를 계산한다.\n",
    "* 각 텐서의 `.grad_fn` 으로 부터 기울기를 계산한다.\n",
    "* 각 텐서의 `.grad` 속성에 계산된 기울기를 누적(accumulate)한다.\n",
    "* 연쇄 법칙(chain rule)을 사용하여 모든 잎leaf 텐서들까지 전파propagate한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_tensor = torch.tensor(5) # 스칼라 텐서\n",
    "vector_tensor = torch.tensor([1,2,3,4]) # 벡터 텐서\n",
    "matrix_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]]) # 행렬 텐서\n",
    "tensor_3d = torch.tensor([[[1, 2, 3], [4, 5, 6]],\n",
    "                          [[7, 8, 9], [10, 11, 12]]]) # 3D 텐서\n",
    "tensor_ones = torch.ones(2, 3) # 2x3, 1로 초기화, 행렬 텐서\n",
    "tensor_zeros = torch.zeros(2, 3) # 2x3, 0으로 초기화, 행렬 텐서\n",
    "tensor_random = torch.randn(2, 3) # 2x3, 정규분포를 따르는 랜덤 값으로 초기화, 행렬 텐서\n",
    "tensor_arange = torch.arange(0, 10) # [0~9]\n",
    "tensor_linspace = torch.linspace(0, 1, steps=5) # [0.0, 0.25, 0.5, 0.75, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([1., 1., 1., 1., 1.]) <class 'torch.Tensor'>\n",
      "y: tensor([0., 0., 0.]) <class 'torch.Tensor'>\n",
      "w: tensor([[ 0.9698, -0.4488, -0.2511],\n",
      "        [ 0.0960, -2.0260, -0.1918],\n",
      "        [-0.6390, -0.2648,  1.2296],\n",
      "        [ 0.9061, -0.0338, -1.8336],\n",
      "        [ 0.5091,  0.0953, -0.0344]], requires_grad=True) <class 'torch.Tensor'>\n",
      "b: tensor([-1.7136,  1.8476, -0.7694], requires_grad=True) <class 'torch.Tensor'>\n",
      "z: tensor([ 0.1285, -0.8305, -1.8507], grad_fn=<AddBackward0>) <class 'torch.Tensor'>\n",
      "\ttorch.matmul: <built-in method matmul of type object at 0x00007FFD01284D40> <class 'builtin_function_or_method'>\n",
      "\tz.grad_fn: <AddBackward0 object at 0x000001298561F580>\n",
      "\tz.grad: None\n",
      "loss: tensor(0.4224, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oiehot\\AppData\\Local\\Temp\\ipykernel_18752\\401830688.py:15: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(\"\\tz.grad:\", z.grad)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b # 1x5 dot 5x3 => 1x3 + 1x3 => 1x3\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "print(\"x:\", x, type(x))\n",
    "print(\"y:\", y, type(y))\n",
    "print(\"w:\", w, type(w))\n",
    "print(\"b:\", b, type(b))\n",
    "print(\"z:\", z, type(z))\n",
    "print(\"\\ttorch.matmul:\", torch.matmul, type(torch.matmul))\n",
    "print(\"\\tz.grad_fn:\", z.grad_fn)\n",
    "print(\"\\tz.grad:\", z.grad)\n",
    "print(\"loss:\", loss, type(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad: tensor([[0.1774, 0.1012, 0.0453],\n",
      "        [0.1774, 0.1012, 0.0453],\n",
      "        [0.1774, 0.1012, 0.0453],\n",
      "        [0.1774, 0.1012, 0.0453],\n",
      "        [0.1774, 0.1012, 0.0453]])\n",
      "b.grad: tensor([0.1774, 0.1012, 0.0453])\n",
      "x.requires_grad: False\n",
      "y.requires_grad: False\n",
      "w.requires_grad: True\n",
      "b.requires_grad: True\n",
      "z.requires_grad: True\n",
      "loss.requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "역전파 계산\n",
    "\n",
    "성능상 이유로 backward는 한 번만 수행되는데 (수행 후 연산그래프가 삭제됨)\n",
    "retain_graph를 True로 설정하면 여러번 수행할 수 있다.\n",
    "\"\"\"\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "print(\"w.grad:\", w.grad)\n",
    "print(\"b.grad:\", b.grad)\n",
    "print(\"x.requires_grad:\", x.requires_grad) # False\n",
    "print(\"y.requires_grad:\", y.requires_grad) # False\n",
    "print(\"w.requires_grad:\", w.requires_grad) # True\n",
    "print(\"b.requires_grad:\", b.requires_grad) # True\n",
    "print(\"z.requires_grad:\", z.requires_grad) # True\n",
    "print(\"loss.requires_grad:\", loss.requires_grad) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "컨텍스트 매니저를 사용하여\n",
    "텐서를 기울기 연산에서 제외하기.\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "\tz = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "detach()를 사용하여 텐서를 기울기 연산에서 제외하기.\n",
    "\"\"\"\n",
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
