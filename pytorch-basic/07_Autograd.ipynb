{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar_tensor = torch.tensor(5) # 스칼라 텐서\n",
    "vector_tensor = torch.tensor([1,2,3,4]) # 벡터 텐서\n",
    "matrix_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]]) # 행렬 텐서\n",
    "tensor_3d = torch.tensor([[[1, 2, 3], [4, 5, 6]],\n",
    "                          [[7, 8, 9], [10, 11, 12]]]) # 3D 텐서\n",
    "tensor_ones = torch.ones(2, 3) # 2x3, 1로 초기화, 행렬 텐서\n",
    "tensor_zeros = torch.zeros(2, 3) # 2x3, 0으로 초기화, 행렬 텐서\n",
    "tensor_random = torch.randn(2, 3) # 2x3, 정규분포를 따르는 랜덤 값으로 초기화, 행렬 텐서\n",
    "tensor_arange = torch.arange(0, 10) # [0~9]\n",
    "tensor_linspace = torch.linspace(0, 1, steps=5) # [0.0, 0.25, 0.5, 0.75, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5)\n",
    "y = torch.zeros(3)\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b # 1x5 dot 5x3 => 1x3 + 1x3 => 1x3\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "print(\"x:\", x, type(x))\n",
    "print(\"y:\", y, type(y))\n",
    "print(\"w:\", w, type(w))\n",
    "print(\"b:\", b, type(b))\n",
    "print(\"z:\", z, type(z))\n",
    "print(\"\\ttorch.matmul:\", torch.matmul, type(torch.matmul))\n",
    "print(\"\\tz.grad_fn:\", z.grad_fn)\n",
    "print(\"\\tz.grad:\", z.grad)\n",
    "print(\"loss:\", loss, type(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "역전파 계산\n",
    "\n",
    "성능상 이유로 backward는 한 번만 수행되는데 (수행 후 연산그래프가 삭제됨)\n",
    "retain_graph를 True로 설정하면 여러번 수행할 수 있다.\n",
    "\"\"\"\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "print(\"w.grad:\", w.grad)\n",
    "print(\"b.grad:\", b.grad)\n",
    "print(\"x.requires_grad:\", x.requires_grad) # False\n",
    "print(\"y.requires_grad:\", y.requires_grad) # False\n",
    "print(\"w.requires_grad:\", w.requires_grad) # True\n",
    "print(\"b.requires_grad:\", b.requires_grad) # True\n",
    "print(\"z.requires_grad:\", z.requires_grad) # True\n",
    "print(\"loss.requires_grad:\", loss.requires_grad) # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "컨텍스트 매니저를 사용하여\n",
    "텐서를 기울기 연산에서 제외하기.\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "\tz = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "detach()를 사용하여 텐서를 기울기 연산에서 제외하기.\n",
    "\"\"\"\n",
    "z = torch.matmul(x, w) + b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
