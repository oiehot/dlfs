{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "def softmax(x):\n",
    "  \"\"\"소프트맥스\n",
    "  \"\"\"\n",
    "  if x.ndim == 2:\n",
    "    x = x.T\n",
    "    x = x - np.max(x, axis=0)\n",
    "    y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    return y.T \n",
    "  x = x - np.max(x) # 오버플로 대책\n",
    "  return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "  \"\"\"교차 엔트로피 오차\n",
    "  \"\"\"\n",
    "  if y.ndim == 1:\n",
    "      t = t.reshape(1, t.size)\n",
    "      y = y.reshape(1, y.size)\n",
    "  if t.size == y.size:\n",
    "      # 원-핫 엔코딩인 경우 정답 레이블의 인덱스 값을 정답으로 사용한다.\n",
    "      t = t.argmax(axis=1)\n",
    "  batch_size = y.shape[0]\n",
    "  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    \"\"\"수치 미분\n",
    "    \"\"\"\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    # np.nditer: 다차원 배열을 반복(iterate)할 때 사용한다.\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) # multi_index 플래그를 주면 it.multi_index 사용 가능해짐.\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index # x가 3차원 배열이면 (i,j,k) 형태의 인덱스를 얻을 수 있다.\n",
    "        tmp_val = x[idx] # A[(0,0)] 인 경우 0행 0번째 항목을 얻을 수 있다.\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        right = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        left = f(x) # f(x-h)\n",
    "        grad[idx] = (right - left) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "1 1.1 200 2.2 110.00000000000001\n"
     ]
    }
   ],
   "source": [
    "class MulLayer:\n",
    "  \"\"\"곱셈 계층\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    self.x = None\n",
    "    self.y = None\n",
    "  \n",
    "  def forward(self, x, y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "    return x * y\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    dx = dout * self.y\n",
    "    dy = dout * self.x\n",
    "    return dx, dy\n",
    "\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax_rate = 1.1\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax_rate)\n",
    "print(price)\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "print(dprice, dapple_price, dtax, dapple, dapple_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple_price: 100\n",
      "apple_num: 2\n",
      "orange_price: 150\n",
      "orange_num: 3\n",
      "tax_rate: 1.1\n",
      "apple_total_price: 200\n",
      "orange_total_price: 450\n",
      "total_price: 650\n",
      "final_price: 715.0000000000001\n",
      "d_final_price: 1\n",
      "d_total_price: 1.1\n",
      "d_tax: 650\n",
      "d_apple_total_price: 1.1\n",
      "d_orange_total_price: 1.1\n",
      "d_apple_price: 2.2\n",
      "d_apple_num: 110.00000000000001\n",
      "d_orange_price: 3.3000000000000003\n",
      "d_orange_num: 165.0\n"
     ]
    }
   ],
   "source": [
    "class AddLayer:\n",
    "  \"\"\"덧셈 계층\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    return x + y\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    dx = dout * 1\n",
    "    dy = dout * 1\n",
    "    return dx, dy\n",
    "\n",
    "apple_price = 100\n",
    "apple_num = 2\n",
    "orange_price = 150\n",
    "orange_num = 3\n",
    "tax_rate = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_price = MulLayer()\n",
    "mul_orange_price = MulLayer()\n",
    "add_apple_orange_price = AddLayer()\n",
    "mul_tax = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_total_price = mul_apple_price.forward(apple_price, apple_num)\n",
    "orange_total_price = mul_orange_price.forward(orange_price, orange_num)\n",
    "total_price = add_apple_orange_price.forward(apple_total_price, orange_total_price)\n",
    "final_price = mul_tax.forward(total_price, tax_rate)\n",
    "print(\"apple_price:\", apple_price)\n",
    "print(\"apple_num:\", apple_num)\n",
    "print(\"orange_price:\", orange_price)\n",
    "print(\"orange_num:\", orange_num)\n",
    "print(\"tax_rate:\", tax_rate)\n",
    "print(\"apple_total_price:\", apple_total_price)\n",
    "print(\"orange_total_price:\", orange_total_price)\n",
    "print(\"total_price:\", total_price)\n",
    "print(\"final_price:\", final_price)\n",
    "\n",
    "# 역전파\n",
    "d_final_price = 1\n",
    "d_total_price, d_tax = mul_tax.backward(d_final_price)\n",
    "d_apple_total_price, d_orange_total_price = add_apple_orange_price.backward(d_total_price)\n",
    "d_apple_price, d_apple_num = mul_apple_price.backward(d_apple_total_price)\n",
    "d_orange_price, d_orange_num = mul_orange_price.backward(d_orange_total_price)\n",
    "print(\"d_final_price:\", d_final_price)\n",
    "print(\"d_total_price:\", d_total_price)\n",
    "print(\"d_tax:\", d_tax)\n",
    "print(\"d_apple_total_price:\", d_apple_total_price)\n",
    "print(\"d_orange_total_price:\", d_orange_total_price)\n",
    "print(\"d_apple_price:\", d_apple_price)\n",
    "print(\"d_apple_num:\", d_apple_num)\n",
    "print(\"d_orange_price:\", d_orange_price)\n",
    "print(\"d_orange_num:\", d_orange_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [-0.2 -0.1  0.   0.1  0.2]\n",
      "rl.forward(a): [0.  0.  0.  0.1 0.2]\n",
      "r1.backward(a): [0. 0. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "class ReluLayer:\n",
    "  \"\"\"ReLU 계층\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self.mask = None\n",
    "  \n",
    "  def forward(self, x):\n",
    "    self.mask = (x <= 0)\n",
    "    out = x.copy()\n",
    "    out[self.mask] = 0\n",
    "    return out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    dout[self.mask] = 0\n",
    "    dx = dout\n",
    "    return dx\n",
    "\n",
    "a = np.array([-0.2, -0.1, 0.0, 0.1, 0.2])\n",
    "dout = np.array([1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "rl = ReluLayer()\n",
    "print(\"a:\", a)\n",
    "print(\"rl.forward(a):\", rl.forward(a))\n",
    "print(\"r1.backward(a):\", rl.backward(dout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [-10.  -5.   0.   5.  10.]\n",
      "sl.forward(a): [4.53978687e-05 6.69285092e-03 5.00000000e-01 9.93307149e-01\n",
      " 9.99954602e-01]\n",
      "s1.backward(a): [4.53958077e-05 6.64805667e-03 2.50000000e-01 6.64805667e-03\n",
      " 4.53958077e-05]\n"
     ]
    }
   ],
   "source": [
    "class SigmoidLayer:\n",
    "  \"\"\"Sigmoid 계층\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    self.out = None\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"순전파\n",
    "    순전파의 출력을 저장해 두고 역전파 때 사용한다.\n",
    "    \"\"\"\n",
    "    self.out = 1 / (1 + np.exp(-x))\n",
    "    return self.out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    dx = dout * self.out * (1-self.out)\n",
    "    return dx\n",
    "\n",
    "a = np.array([-10.0, -5.0, 0.0, 5.0, 10.0])\n",
    "dout = np.array([1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "sl = SigmoidLayer()\n",
    "print(\"a:\", a)\n",
    "print(\"sl.forward(a):\", sl.forward(a))\n",
    "print(\"s1.backward(a):\", sl.backward(dout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[1 2 3]\n",
      " [4 5 6]]\n",
      "A.T: [[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "X: [[1 2 3]\n",
      " [4 5 6]]\n",
      "W: [[0.1 0.2 0.3]\n",
      " [0.4 0.5 0.6]\n",
      " [0.7 0.8 0.9]]\n",
      "B: [0.1 0.2 0.3]\n",
      "D: [[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "al_forward_out: [[3.1 3.8 4.5]\n",
      " [6.7 8.3 9.9]]\n",
      "al_backward_out: [[0.6 1.5 2.4]\n",
      " [0.6 1.5 2.4]]\n"
     ]
    }
   ],
   "source": [
    "class AffineLayer:\n",
    "  \"\"\"Affine 계층\n",
    "  \"\"\"\n",
    "  def __init__(self, W, b):\n",
    "    self.X = None\n",
    "    self.W = W\n",
    "    self.b = b\n",
    "    self.dW = None\n",
    "    self.db = None\n",
    "    pass\n",
    "\n",
    "  def forward(self, X):\n",
    "    \"\"\"순전파\n",
    "    \"\"\"\n",
    "    self.X = X\n",
    "    return np.dot(X, self.W) + self.b\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    \"\"\"역전파\n",
    "    \"\"\"\n",
    "    dX = np.dot(dout, self.W.T)\n",
    "    self.dW = np.dot(self.X.T, dout)\n",
    "    self.db = np.sum(dout, axis=0)\n",
    "    return dX\n",
    "\n",
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "A.T\n",
    "print(\"A:\", A)\n",
    "print(\"A.T:\" , A.T)\n",
    "X = np.array([[1,2,3], [4,5,6]])\n",
    "W = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]])\n",
    "b = np.array([0.1, 0.2, 0.3])\n",
    "D = np.array([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0]])\n",
    "al = AffineLayer(W, b)\n",
    "al_forward_out = al.forward(X)\n",
    "al_backward_out = al.backward(D)\n",
    "print(\"X:\", X)\n",
    "print(\"W:\", W)\n",
    "print(\"B:\", b)\n",
    "print(\"D:\", D)\n",
    "print(\"al_forward_out:\", al_forward_out)\n",
    "print(\"al_backward_out:\", al_backward_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [[1 2 3]\n",
      " [4 5 6]]\n",
      "T: [[0 1 0]\n",
      " [1 0 0]]\n",
      "Y: [[0.09003057 0.24472847 0.66524096]\n",
      " [0.09003057 0.24472847 0.66524096]]\n",
      "loss: 1.9076052047697707\n",
      "dx: [[ 0.04501529 -0.37763576  0.33262048]\n",
      " [-0.45498471  0.12236424  0.33262048]]\n"
     ]
    }
   ],
   "source": [
    "class SoftmaxWithLoss:\n",
    "  \"\"\"Softmax with Loss(cross entropy error) 계층\n",
    "  \"\"\"\n",
    "\n",
    "  def __init(self):\n",
    "    self.y = None # softmax의 출력\n",
    "    self.t = None # 정답 레이블(원-핫 엔코딩)\n",
    "    self.loss = None # 손실\n",
    "\n",
    "  def forward(self, x, t):\n",
    "    \"\"\"순전파\n",
    "    x: 입력값\n",
    "    t: 정답 레이블 (원-핫 엔코딩)\n",
    "    \"\"\"\n",
    "    self.t = t\n",
    "    self.y = softmax(x)\n",
    "    self.loss = cross_entropy_error(self.y, self.t)\n",
    "    return self.loss\n",
    "\n",
    "  def backward(self, dout=1):\n",
    "    batch_size = self.t.shape[0]\n",
    "    dx = (self.y - self.t) / batch_size\n",
    "    return dx\n",
    "\n",
    "X = np.array([[1,2,3], [4,5,6]])\n",
    "T = np.array([[0,1,0], [1,0,0]])\n",
    "sl = SoftmaxWithLoss()\n",
    "loss = sl.forward(X, T)\n",
    "dx = sl.backward()\n",
    "print(\"X:\", X)\n",
    "print(\"T:\", T)\n",
    "print(\"Y:\", sl.y)\n",
    "print(\"loss:\", loss)\n",
    "print(\"dx:\", dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "  \"\"\"MNIST 훈련 및 추론을 위한 2층 신경망.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "    # 가중치\n",
    "    self.params = {}\n",
    "    self.params[\"W1\"] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "    self.params[\"b1\"] = np.zeros(hidden_size)\n",
    "    self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "    self.params[\"b2\"] = np.zeros(output_size)\n",
    "\n",
    "    # 계층\n",
    "    self.layers = OrderedDict()\n",
    "    self.layers[\"Affine1\"] = AffineLayer(self.params[\"W1\"], self.params[\"b1\"])\n",
    "    self.layers[\"Relu1\"]   = ReluLayer()\n",
    "    self.layers[\"Affine2\"] = AffineLayer(self.params[\"W2\"], self.params[\"b2\"])\n",
    "    self.last_layer = SoftmaxWithLoss()\n",
    "    \n",
    "  def predict(self, x):\n",
    "    \"\"\"추론.\n",
    "    입력값을 신경망 순전파 처리.\n",
    "    \"\"\"\n",
    "    for layer in self.layers.values():\n",
    "      x = layer.forward(x)\n",
    "    return x\n",
    "  \n",
    "  def loss(self, x, t):\n",
    "    \"\"\"추론 후 손실함수 처리.\n",
    "    \"\"\"\n",
    "    y = self.predict(x)\n",
    "    return self.last_layer.forward(y, t)\n",
    "  \n",
    "  def accuracy(self, x, t):\n",
    "    \"\"\"입력에 대한 추론 결과와 정답지를 비교하여 평균 정답률 계산하기.\n",
    "    \"\"\"\n",
    "    y = self.predict(x)\n",
    "    y = np.argmax(y, axis=1) # 열 방향 데이터 중 가장 큰 값의 인덱스를 얻기.\n",
    "    if t.ndim != 1:\n",
    "      t = np.argmax(t, axis=1) # 원핫엔코딩 데이터를 정답값(숫자)로 변환하기 위해 가장 큰 값의 인덱스를 얻음\n",
    "    accuracy = np.sum(y == t) / float(x.shape[0]) # 정답을 맞춘 수를 데이터 수(행 수)로 나눠 평균을 냄.\n",
    "    return accuracy\n",
    "\n",
    "  def numerical_gradient(self, x, t):\n",
    "    \"\"\"수치미분 방식으로 기울기 계산하기.\n",
    "    \"\"\"\n",
    "    loss_W = lambda W: self.loss(x, t)\n",
    "    grads = {}\n",
    "    grads[\"W1\"] = numerical_gradient(loss_W, self.params[\"W1\"])\n",
    "    grads[\"b1\"] = numerical_gradient(loss_W, self.params[\"b1\"])\n",
    "    grads[\"W2\"] = numerical_gradient(loss_W, self.params[\"W2\"])\n",
    "    grads[\"b2\"] = numerical_gradient(loss_W, self.params[\"b2\"])\n",
    "    return grads\n",
    "  \n",
    "  def gradient(self, x, t):\n",
    "    \"\"\"역전파 방식으로 기울기 계산하기.\n",
    "    \"\"\"\n",
    "\n",
    "    # 순전파 신경망 계산을 한다.\n",
    "    self.loss(x, t)\n",
    "\n",
    "    dout = 1\n",
    "    dout = self.last_layer.backward(dout)\n",
    "    \n",
    "    # A. 레이어의 순서를 변경한 리스트를 만들어 역방향으로 backward메서드를 실행하여 역전파로 미분값을 얻는다.\n",
    "    # layers = list(self.layers.values())\n",
    "    # layers.reverse()\n",
    "    # for layer in layers:\n",
    "    #   dout = layer.backward(dout)\n",
    "\n",
    "    # B. OrderedList의 역방향 이터레이터를 사용, 레이어 역순으로 backward 메서드를 호출한다.\n",
    "    for key in reversed(self.layers):\n",
    "      dout = self.layers[key].backward(dout)\n",
    "    \n",
    "    # 미분 결과(기울기)를 저장한다.\n",
    "    grads = {}\n",
    "    grads[\"W1\"] = self.layers[\"Affine1\"].dW\n",
    "    grads[\"b1\"] = self.layers[\"Affine1\"].db\n",
    "    grads[\"W2\"] = self.layers[\"Affine2\"].dW\n",
    "    grads[\"b2\"] = self.layers[\"Affine2\"].db\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : 4.545661952811881e-10\n",
      "b1 : 3.0766799402777692e-09\n",
      "W2 : 6.468307528790065e-09\n",
      "b2 : 1.4066052205774505e-07\n"
     ]
    }
   ],
   "source": [
    "\"\"\"수치미분과 역전파방식 미분의 결과 차이를 통해\n",
    "역전파를 통해 구한 결과의 정확도를 확인하기.\n",
    "\"\"\"\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=28*28, hidden_size=50, output_size=10)\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop  = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "  diff = np.average(np.abs(\n",
    "    grad_backprop[key] - grad_numerical[key]\n",
    "  ))\n",
    "  print(key,\":\",str(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 / train_accuracy: 0.09973333333333333, test_accuracy: 0.104\n",
      "iter 600 / train_accuracy: 0.5220833333333333, test_accuracy: 0.5241\n",
      "iter 1200 / train_accuracy: 0.7736166666666666, test_accuracy: 0.7807\n",
      "iter 1800 / train_accuracy: 0.84505, test_accuracy: 0.8499\n",
      "iter 2400 / train_accuracy: 0.8714333333333333, test_accuracy: 0.8743\n",
      "iter 3000 / train_accuracy: 0.8839333333333333, test_accuracy: 0.8876\n",
      "iter 3600 / train_accuracy: 0.8923333333333333, test_accuracy: 0.8967\n",
      "iter 4200 / train_accuracy: 0.8969166666666667, test_accuracy: 0.9002\n",
      "iter 4800 / train_accuracy: 0.9007666666666667, test_accuracy: 0.9044\n",
      "iter 5400 / train_accuracy: 0.9042833333333333, test_accuracy: 0.9082\n",
      "iter 6000 / train_accuracy: 0.90705, test_accuracy: 0.9096\n",
      "iter 6600 / train_accuracy: 0.90935, test_accuracy: 0.9126\n",
      "iter 7200 / train_accuracy: 0.91145, test_accuracy: 0.9145\n",
      "iter 7800 / train_accuracy: 0.9139833333333334, test_accuracy: 0.9169\n",
      "iter 8400 / train_accuracy: 0.9158833333333334, test_accuracy: 0.9186\n",
      "iter 9000 / train_accuracy: 0.9168833333333334, test_accuracy: 0.9197\n",
      "iter 9600 / train_accuracy: 0.91955, test_accuracy: 0.9217\n",
      "iter 10200 / train_accuracy: 0.9202666666666667, test_accuracy: 0.9238\n",
      "iter 10800 / train_accuracy: 0.9218666666666666, test_accuracy: 0.9253\n",
      "iter 11400 / train_accuracy: 0.92335, test_accuracy: 0.9266\n",
      "iter 12000 / train_accuracy: 0.9253666666666667, test_accuracy: 0.9276\n",
      "iter 12600 / train_accuracy: 0.92595, test_accuracy: 0.9298\n",
      "iter 13200 / train_accuracy: 0.9273166666666667, test_accuracy: 0.9303\n",
      "iter 13800 / train_accuracy: 0.9284, test_accuracy: 0.93\n",
      "iter 14400 / train_accuracy: 0.9292833333333334, test_accuracy: 0.9334\n",
      "iter 15000 / train_accuracy: 0.9315333333333333, test_accuracy: 0.9331\n",
      "iter 15600 / train_accuracy: 0.9320166666666667, test_accuracy: 0.9334\n",
      "iter 16200 / train_accuracy: 0.9334833333333333, test_accuracy: 0.9347\n",
      "iter 16800 / train_accuracy: 0.9349333333333333, test_accuracy: 0.9369\n",
      "iter 17400 / train_accuracy: 0.9357333333333333, test_accuracy: 0.9356\n",
      "iter 18000 / train_accuracy: 0.9373833333333333, test_accuracy: 0.9364\n",
      "iter 18600 / train_accuracy: 0.9378, test_accuracy: 0.9376\n",
      "iter 19200 / train_accuracy: 0.9386166666666667, test_accuracy: 0.938\n",
      "iter 19800 / train_accuracy: 0.9388833333333333, test_accuracy: 0.9379\n",
      "iter 20400 / train_accuracy: 0.9407333333333333, test_accuracy: 0.9397\n",
      "iter 21000 / train_accuracy: 0.9414666666666667, test_accuracy: 0.9394\n",
      "iter 21600 / train_accuracy: 0.9420666666666667, test_accuracy: 0.9402\n",
      "iter 22200 / train_accuracy: 0.9429666666666666, test_accuracy: 0.9411\n",
      "iter 22800 / train_accuracy: 0.94355, test_accuracy: 0.9415\n",
      "iter 23400 / train_accuracy: 0.9446, test_accuracy: 0.9422\n",
      "iter 24000 / train_accuracy: 0.94535, test_accuracy: 0.942\n",
      "iter 24600 / train_accuracy: 0.9454333333333333, test_accuracy: 0.9431\n",
      "iter 25200 / train_accuracy: 0.9464166666666667, test_accuracy: 0.9431\n",
      "iter 25800 / train_accuracy: 0.947, test_accuracy: 0.944\n",
      "iter 26400 / train_accuracy: 0.9475666666666667, test_accuracy: 0.9446\n",
      "iter 27000 / train_accuracy: 0.9482333333333334, test_accuracy: 0.945\n",
      "iter 27600 / train_accuracy: 0.94955, test_accuracy: 0.9447\n",
      "iter 28200 / train_accuracy: 0.9500666666666666, test_accuracy: 0.9453\n",
      "iter 28800 / train_accuracy: 0.9504833333333333, test_accuracy: 0.9461\n",
      "iter 29400 / train_accuracy: 0.9507666666666666, test_accuracy: 0.9461\n",
      "iter 30000 / train_accuracy: 0.9517166666666667, test_accuracy: 0.9463\n",
      "iter 30600 / train_accuracy: 0.9521166666666666, test_accuracy: 0.9478\n",
      "iter 31200 / train_accuracy: 0.95285, test_accuracy: 0.9478\n",
      "iter 31800 / train_accuracy: 0.9529833333333333, test_accuracy: 0.9487\n",
      "iter 32400 / train_accuracy: 0.9538833333333333, test_accuracy: 0.9486\n",
      "iter 33000 / train_accuracy: 0.9543666666666667, test_accuracy: 0.949\n",
      "iter 33600 / train_accuracy: 0.9550666666666666, test_accuracy: 0.9497\n",
      "iter 34200 / train_accuracy: 0.9555833333333333, test_accuracy: 0.9503\n",
      "iter 34800 / train_accuracy: 0.9553, test_accuracy: 0.9505\n",
      "iter 35400 / train_accuracy: 0.95625, test_accuracy: 0.9515\n",
      "iter 36000 / train_accuracy: 0.9570333333333333, test_accuracy: 0.9518\n",
      "iter 36600 / train_accuracy: 0.9571666666666667, test_accuracy: 0.9522\n",
      "iter 37200 / train_accuracy: 0.9578, test_accuracy: 0.9533\n",
      "iter 37800 / train_accuracy: 0.9584666666666667, test_accuracy: 0.9531\n",
      "iter 38400 / train_accuracy: 0.9585666666666667, test_accuracy: 0.9529\n",
      "iter 39000 / train_accuracy: 0.95935, test_accuracy: 0.953\n",
      "iter 39600 / train_accuracy: 0.9596666666666667, test_accuracy: 0.9531\n",
      "iter 40200 / train_accuracy: 0.9600333333333333, test_accuracy: 0.9534\n",
      "iter 40800 / train_accuracy: 0.9603166666666667, test_accuracy: 0.9551\n",
      "iter 41400 / train_accuracy: 0.9606333333333333, test_accuracy: 0.9551\n",
      "iter 42000 / train_accuracy: 0.96105, test_accuracy: 0.9549\n",
      "iter 42600 / train_accuracy: 0.9616833333333333, test_accuracy: 0.9561\n",
      "iter 43200 / train_accuracy: 0.9621166666666666, test_accuracy: 0.9559\n",
      "iter 43800 / train_accuracy: 0.9624666666666667, test_accuracy: 0.9567\n",
      "iter 44400 / train_accuracy: 0.9627833333333333, test_accuracy: 0.9563\n",
      "iter 45000 / train_accuracy: 0.9629333333333333, test_accuracy: 0.957\n",
      "iter 45600 / train_accuracy: 0.9628666666666666, test_accuracy: 0.9572\n",
      "iter 46200 / train_accuracy: 0.9634833333333334, test_accuracy: 0.9573\n",
      "iter 46800 / train_accuracy: 0.9639833333333333, test_accuracy: 0.9579\n",
      "iter 47400 / train_accuracy: 0.96435, test_accuracy: 0.9591\n",
      "iter 48000 / train_accuracy: 0.9643833333333334, test_accuracy: 0.9585\n",
      "iter 48600 / train_accuracy: 0.9650166666666666, test_accuracy: 0.9595\n",
      "iter 49200 / train_accuracy: 0.9653833333333334, test_accuracy: 0.9596\n",
      "iter 49800 / train_accuracy: 0.96585, test_accuracy: 0.9597\n",
      "iter 50400 / train_accuracy: 0.9662166666666666, test_accuracy: 0.9601\n",
      "iter 51000 / train_accuracy: 0.9661666666666666, test_accuracy: 0.96\n",
      "iter 51600 / train_accuracy: 0.96665, test_accuracy: 0.9601\n",
      "iter 52200 / train_accuracy: 0.9668833333333333, test_accuracy: 0.9612\n",
      "iter 52800 / train_accuracy: 0.9672833333333334, test_accuracy: 0.961\n",
      "iter 53400 / train_accuracy: 0.96735, test_accuracy: 0.9616\n",
      "iter 54000 / train_accuracy: 0.9677333333333333, test_accuracy: 0.9608\n",
      "iter 54600 / train_accuracy: 0.9682333333333333, test_accuracy: 0.9611\n",
      "iter 55200 / train_accuracy: 0.9685666666666667, test_accuracy: 0.9625\n",
      "iter 55800 / train_accuracy: 0.96885, test_accuracy: 0.962\n",
      "iter 56400 / train_accuracy: 0.9690666666666666, test_accuracy: 0.9615\n",
      "iter 57000 / train_accuracy: 0.9692, test_accuracy: 0.9623\n",
      "iter 57600 / train_accuracy: 0.9696833333333333, test_accuracy: 0.9618\n",
      "iter 58200 / train_accuracy: 0.9697833333333333, test_accuracy: 0.9624\n",
      "iter 58800 / train_accuracy: 0.9693833333333334, test_accuracy: 0.9629\n",
      "iter 59400 / train_accuracy: 0.9697666666666667, test_accuracy: 0.9629\n",
      "iter 60000 / train_accuracy: 0.97, test_accuracy: 0.9631\n",
      "iter 60600 / train_accuracy: 0.9703166666666667, test_accuracy: 0.9624\n",
      "iter 61200 / train_accuracy: 0.9707666666666667, test_accuracy: 0.9634\n",
      "iter 61800 / train_accuracy: 0.9706166666666667, test_accuracy: 0.9644\n",
      "iter 62400 / train_accuracy: 0.9712, test_accuracy: 0.9637\n",
      "iter 63000 / train_accuracy: 0.9714, test_accuracy: 0.9636\n",
      "iter 63600 / train_accuracy: 0.97125, test_accuracy: 0.9637\n",
      "iter 64200 / train_accuracy: 0.9717833333333333, test_accuracy: 0.9644\n",
      "iter 64800 / train_accuracy: 0.9720333333333333, test_accuracy: 0.9648\n",
      "iter 65400 / train_accuracy: 0.9721, test_accuracy: 0.9643\n",
      "iter 66000 / train_accuracy: 0.9725833333333334, test_accuracy: 0.9645\n",
      "iter 66600 / train_accuracy: 0.9729, test_accuracy: 0.9654\n",
      "iter 67200 / train_accuracy: 0.9730166666666666, test_accuracy: 0.9657\n",
      "iter 67800 / train_accuracy: 0.9730166666666666, test_accuracy: 0.9655\n",
      "iter 68400 / train_accuracy: 0.9733333333333334, test_accuracy: 0.9651\n",
      "iter 69000 / train_accuracy: 0.9732166666666666, test_accuracy: 0.9653\n",
      "iter 69600 / train_accuracy: 0.9737666666666667, test_accuracy: 0.9664\n",
      "iter 70200 / train_accuracy: 0.9740666666666666, test_accuracy: 0.9653\n",
      "iter 70800 / train_accuracy: 0.9739666666666666, test_accuracy: 0.966\n",
      "iter 71400 / train_accuracy: 0.9744166666666667, test_accuracy: 0.9659\n",
      "iter 72000 / train_accuracy: 0.9747333333333333, test_accuracy: 0.9663\n",
      "iter 72600 / train_accuracy: 0.9742833333333333, test_accuracy: 0.9663\n",
      "iter 73200 / train_accuracy: 0.9748, test_accuracy: 0.9663\n",
      "iter 73800 / train_accuracy: 0.97475, test_accuracy: 0.9669\n",
      "iter 74400 / train_accuracy: 0.9751666666666666, test_accuracy: 0.9665\n",
      "iter 75000 / train_accuracy: 0.9753166666666667, test_accuracy: 0.9668\n",
      "iter 75600 / train_accuracy: 0.9755833333333334, test_accuracy: 0.9673\n",
      "iter 76200 / train_accuracy: 0.9752833333333333, test_accuracy: 0.9666\n",
      "iter 76800 / train_accuracy: 0.9757, test_accuracy: 0.967\n",
      "iter 77400 / train_accuracy: 0.9757, test_accuracy: 0.9666\n",
      "iter 78000 / train_accuracy: 0.9760833333333333, test_accuracy: 0.9672\n",
      "iter 78600 / train_accuracy: 0.9760166666666666, test_accuracy: 0.9683\n",
      "iter 79200 / train_accuracy: 0.9763, test_accuracy: 0.9677\n",
      "iter 79800 / train_accuracy: 0.9767166666666667, test_accuracy: 0.9683\n",
      "iter 80400 / train_accuracy: 0.9768833333333333, test_accuracy: 0.9685\n",
      "iter 81000 / train_accuracy: 0.9770833333333333, test_accuracy: 0.9686\n",
      "iter 81600 / train_accuracy: 0.97725, test_accuracy: 0.9677\n",
      "iter 82200 / train_accuracy: 0.9776833333333333, test_accuracy: 0.9682\n",
      "iter 82800 / train_accuracy: 0.9776833333333333, test_accuracy: 0.9681\n",
      "iter 83400 / train_accuracy: 0.9778, test_accuracy: 0.9688\n",
      "iter 84000 / train_accuracy: 0.9780666666666666, test_accuracy: 0.9686\n",
      "iter 84600 / train_accuracy: 0.9780833333333333, test_accuracy: 0.9696\n",
      "iter 85200 / train_accuracy: 0.9780333333333333, test_accuracy: 0.9696\n",
      "iter 85800 / train_accuracy: 0.9777833333333333, test_accuracy: 0.9688\n",
      "iter 86400 / train_accuracy: 0.9788, test_accuracy: 0.9686\n",
      "iter 87000 / train_accuracy: 0.97835, test_accuracy: 0.9689\n",
      "iter 87600 / train_accuracy: 0.9789333333333333, test_accuracy: 0.9698\n",
      "iter 88200 / train_accuracy: 0.97905, test_accuracy: 0.97\n",
      "iter 88800 / train_accuracy: 0.9791833333333333, test_accuracy: 0.97\n",
      "iter 89400 / train_accuracy: 0.97945, test_accuracy: 0.9693\n",
      "iter 90000 / train_accuracy: 0.9790666666666666, test_accuracy: 0.969\n",
      "iter 90600 / train_accuracy: 0.9792, test_accuracy: 0.9688\n",
      "iter 91200 / train_accuracy: 0.9797, test_accuracy: 0.9703\n",
      "iter 91800 / train_accuracy: 0.9795666666666667, test_accuracy: 0.9701\n",
      "iter 92400 / train_accuracy: 0.97965, test_accuracy: 0.9704\n",
      "iter 93000 / train_accuracy: 0.9796833333333334, test_accuracy: 0.9704\n",
      "iter 93600 / train_accuracy: 0.9796666666666667, test_accuracy: 0.9696\n",
      "iter 94200 / train_accuracy: 0.9799833333333333, test_accuracy: 0.9705\n",
      "iter 94800 / train_accuracy: 0.9802666666666666, test_accuracy: 0.9703\n",
      "iter 95400 / train_accuracy: 0.9795833333333334, test_accuracy: 0.9699\n",
      "iter 96000 / train_accuracy: 0.9804166666666667, test_accuracy: 0.9702\n",
      "iter 96600 / train_accuracy: 0.9804666666666667, test_accuracy: 0.9702\n",
      "iter 97200 / train_accuracy: 0.9804833333333334, test_accuracy: 0.9705\n",
      "iter 97800 / train_accuracy: 0.9805333333333334, test_accuracy: 0.9707\n",
      "iter 98400 / train_accuracy: 0.9804666666666667, test_accuracy: 0.9716\n",
      "iter 99000 / train_accuracy: 0.9806166666666667, test_accuracy: 0.971\n",
      "iter 99600 / train_accuracy: 0.98075, test_accuracy: 0.9708\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "train_loss_list = []\n",
    "train_accuracy_list = []\n",
    "test_accuracy_list = []\n",
    "iters_num = 100_000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "network = TwoLayerNet(input_size=28*28, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "  # 미니배치용 데이터 준비.\n",
    "  batch_mask = np.random.choice(train_size, batch_size)\n",
    "  x_batch = x_train[batch_mask]\n",
    "  t_batch = t_train[batch_mask]\n",
    "\n",
    "  # 기울기 얻기.\n",
    "  # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "  grad = network.gradient(x_batch, t_batch) # 오차역전파법을 이용해 기울기를 구하는 함수.\n",
    "\n",
    "  # 기울기를 이용해 경사하강법으로 손실함수의 결과값을 줄이는 방향으로 각 파라미터(가중치와 편향) 조정하기.\n",
    "  for key in (\"W1\", \"b1\", \"W2\", \"b2\"):\n",
    "    # 기울기 값이 음수면 손실함수의 결과가 0으로 향하기 위해 더해줌.\n",
    "    network.params[key] -= learning_rate * grad[key]\n",
    "  \n",
    "  # 학습경과를 파악하기 위해\n",
    "  # 수정된 파라미터를 통해 손실함수를 계산하고 그 결과를 보관하기.\n",
    "  loss = network.loss(x_batch, t_batch)\n",
    "  train_loss_list.append(loss)\n",
    "\n",
    "  if i % iter_per_epoch == 0:\n",
    "    train_accuracy = network.accuracy(x_train, t_train)\n",
    "    test_accuracy  = network.accuracy(x_test, t_test)\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "    print(f\"iter {i} / train_accuracy: {train_accuracy}, test_accuracy: {test_accuracy}\")\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
